---
layout: post
title: "Eulerian Video Magnification basics"
description: "This is my GSOC2015 log"
category: GSOC2015
tags: [Matlab,VTK]
---
{% include JB/setup %}

This post indroduce the basics of the EVM(Eulerian Video Magnification) Algorithm. For details: [EVM](http://people.csail.mit.edu/mrub/evm/).

***
    
1. ###EVM Preview
   Evm is the abbreviation of *Eulerian Video Magnification*. According to the [paper](https://googledrive.com/host/0B6Io4fF4zXvDTnBFeXRBM0Vja3c/images/gsoc_Basics/vidmag.pdf), that:

   > Out method, which we call Eulerian Video Magnification, takes a standard video sequence as input, and applies spatial decoposition signal, followed by temporal filtering to the frames. The resulting signal is then amplified to reveal hidden information.
   
   And as it is said in the paper, that EVM **relies on a linear approximation related to the brightness contancy assumption used in optical flow**. As a result:

   > Using our method, we are able to visualize the flow of blood as it fills the face and also to amplify and reveal small motions.

   EVM is both applicable to *color variation* and *low-amplitude motion* but require different processing techniques.

   EVM is inspired by *Eulerian* perspective, which is in constrast to *Lagrangian* perspective.

   * For *Lagrangian* perspective, it is **in reference to fluid dynamics where the trajectory of particles is tracked over time**. And it **rely on accurate motion estimation** and other additional techniques, including **motion segmentation, image in-painting**, and are required to **produce good quality synthesis**.
   <figure>
   	<img src="https://googledrive.com/host/0B6Io4fF4zXvDTnBFeXRBM0Vja3c/images/gsoc_Basics/1948ulu.png" alt="Lagrangian perspective">
   	<figcaption align='middle'><b>Lagrangian perspective</b></figcaption>
   	</figure>
   * In contrast, for *Eulerian* perspective, it is **properties of a voxel of fluid, such as pressure and velocity, evolve over time**. **We amplify the variation of pixel values over time, in a spatially-multiscale manner**. **We do not explicityly estimate motion, but rather exaggerate motion by amplifying temporal color changes at fixed position**.
   <figure>
   	<img src="https://googledrive.com/host/0B6Io4fF4zXvDTnBFeXRBM0Vja3c/images/gsoc_Basics/1839Kmo.png" alt="Eulerian perspective">
   	<figcaption align='middle'><b>Eulerian perspective</b></figcaption>
   </figure>
      
   In a word, *Eulerian* perspective is a easier mothod.


+. ###Technical Lists

   <figure>
   	<img src="https://googledrive.com/host/0B6Io4fF4zXvDTnBFeXRBM0Vja3c/images/gsoc_Basics/19487v0-1.png" alt="EVM">
   	<figcaption align='middle'><b>EVM</b></figcaption>
   </figure>
 
 
   * First, we do spatial frequency decomposition. For each band they might be magnified differently because:
      * They might exhibit different S/N ratios.
      * They might contain spatial frequencies for which the linear approximation used in the motion magnification does not hold.
       
     If the goal of spatial processing is simply to increase temporal S/N ratio by pooling multiple pixels, we spatially low-pass filter the frames of the video and simply downsample them for computational efficiency.  
   
   * Second, There are four steps to take for each spatial frequency bands:
  
     * Select a temporal bandpass filter
     * Select an amplification factor, \\( \alpha \\).
     * Select a spatial frequency cutoff (specified by spatial wavelength \\( \lambda_c \\) beyond which an attenuated version of \\( \alpha \\) is used.
     * Select the form of the attenuation for \\( \alpha \\)-either force \\( \alpha \\) to zero for all \\( \lambda < \lambda_c \\), or linearly scale \\( \alpha \\) down to zero.
    
   * Third, adding the magnified signal to the original and collapse the spatial pyramid will get us to the final result. 
     
     
    One thing to figure out, **Since natural videos are spatially and temporally smooth, and since filtering is persormed uniformly over the pixels, the method implicitly maintains spatiotemporal coherency**.
   

+ ### Spatial Decomposition
   Spatial decomposition is the standard process of *building Gussian pyramid* or *Laplacian pyramid*.
   
	<ol>
  		<li>Reduce: <figure>
   		<img src="https://googledrive.com/host/0B6Io4fF4zXvDTnBFeXRBM0Vja3c/images/		2015/3/2015-03-28_21-55-13.png" alt="Eulerian perspective">
   		<figcaption align='middle'><b>Reduce</b></figcaption>
   		</figure>
   		</li>
  		
  		<li>expand: <figure>
		<img src="https://googledrive.com/host/0B6Io4fF4zXvDTnBFeXRBM0Vja3c/images/		2015/3/2015-03-28_21-55-13.png">
		<figcaption align='middle'><b>expand</b></figcaption>
		</figure>
		</li>

  		<li>Laplacian pyramid: <figure>
		<img src="https://googledrive.com/host/0B6Io4fF4zXvDTnBFeXRBM0Vja3c/images/		2015/3/2015-03-28_21-55-21.png">
		<figcaption align='middle'><b>Laplacian pyramid</b></figcaption>
		</figure></li>
	</ol>
	
	
	The process of creating Gaussian Pyramid is the process of doing convolution with the low pass filter, we loss information along with the creation of the pyramid. But at the same time we get a more generalized image, an image with less noise. And the information we lost is in the Laplacian pyramid.    

+ ### Temporal Filtering
	Filtering is the process of getting the desired bandpass signal:
	i.e.
	
	$$ B(x,t) = \delta* \frac{\partial f(x)}{\partial x} $$

  So, we just assume that after the filter we can get the desired frequency( We assume that the pixel color changes along with the desired properties, i.e. human breath 60 times a minute, we assume the pixel color has the similar behavior)(The paper said if not so we can separate it out and amplify the individual frequency separately)

  We can use,
  * IIR Second order filter
  * Butterworth filter
  * Ideal bandpass filter
  * etc..

+ ### Combining Them Up
  Now we already had the material to work on: The pyramid generalized image, from which we also have subtracted wanted bands. Time to summing them up, 
  The idea of amplify is very simple, multiple it and add it back( This is called first order approximation). 

  However, amplification also comes with artifact, to illustrate this effect, I created an png animation, I crated an testing moving sine wave, and I want to amplify it. 

  I created a demo of which:
  <figure>
  <img src="https://googledrive.com/host/0B6Io4fF4zXvDTnBFeXRBM0Vja3c/images/2015/3/hope.gif">
  <figcaption align='middle'><b>Demo</b></figcaption>
  </figure>

  As we can see from the image, the one in the red line is the amplified signal, the one in the blue line is the signal start point, the one in the green the signal in the next time step. The amplifying ratio is kept at 1.

  If we keeping increase the time step, for the constant amplifying ratio, the artifact effect keeping increasing( We want to replace the blue one with the red one)

  Although this is only a demo for time steps, but as we can see from the paper, that smaller time step allows bigger amplifying ratio. So if you want to avoid artifact we would have to have a guide line. 

  Function 14 in the paper provided with us with such formula:
  <figure>
  <img src="https://googledrive.com/host/0B6Io4fF4zXvDTnBFeXRBM0Vja3c/images/2015/3/2015-03-29_00-24-22.png">
  <figcaption align='middle'><b>Guideline</b></figcaption>
  </figure>

+ ### What's next?
  Now we have got mask for the original signal, this mark is supposed to have the ability of amplify the changes in the preset frequency, we only need to superimpose the mask onto the original video.

  See the next tutorial for the source code of this algorithm.
  





  

